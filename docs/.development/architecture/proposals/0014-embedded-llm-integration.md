# Embedded LLM Integration for Rhema - Revised 2025

**Proposal**: Extend Rhema with embedded LLM capabilities to enable local AI processing for small tasks while maintaining the ability to offload complex tasks to paid external APIs or remote runners via task scoring.

**Status**: üîÑ **In Progress** - Core AI service infrastructure exists, embedded LLM integration pending  
**Priority**: High  
**Effort**: 8-12 weeks (reduced from original 16 weeks due to existing infrastructure)

## Executive Summary

This proposal has been significantly revised to reflect the current state of Rhema's AI service architecture and recent developments in embedded LLM technology. The original proposal was written when Rhema had a basic AI service, but now we have a sophisticated AI service with lock file awareness, agent state management, coordination integration, and advanced conflict prevention systems.

## Current State Analysis

### What's Already Implemented ‚úÖ

Based on the current codebase analysis, Rhema now has:

1. **Advanced AI Service Architecture** (`crates/rhema-coordination/src/ai_service.rs`):
   - Sophisticated `AIService` with lock file awareness
   - Agent state management and coordination integration
   - Advanced conflict prevention systems
   - Comprehensive metrics and monitoring
   - Multi-provider support (OpenAI, Anthropic)

2. **MCP Daemon Infrastructure** (`crates/rhema-mcp/src/mcp.rs`):
   - Robust daemon with authentication, caching, and monitoring
   - HTTP server, WebSocket, and Unix socket support
   - File watching and context provider integration
   - Performance metrics and health monitoring

3. **Knowledge Management System** (`crates/rhema-knowledge/`):
   - Production-ready RAG system with semantic search
   - Multi-tier caching (memory, disk, network)
   - Advanced embedding system with multiple models
   - Intelligent indexing and proactive features

4. **Coordination and Agent Systems**:
   - Syneidesis coordination library integration
   - Real-time agent communication
   - Task scoring and constraint systems
   - Advanced conflict prevention

### What's Missing for Embedded LLM ‚ùå

1. **Embedded Model Infrastructure**: No local model loading or inference capabilities
2. **Task Classification System**: No intelligent routing between embedded and external models
3. **Model Management**: No model downloading, updating, or validation systems
4. **Hybrid Processing**: No combination of embedded and external processing
5. **Performance Optimization**: No embedded model-specific optimizations

## Problem Statement (Updated)

While Rhema's AI service architecture has evolved significantly, it still relies entirely on external API calls, which presents several limitations:

- **Cost Inefficiency**: All AI processing incurs external API costs, even for simple tasks like context queries and validation
- **Latency Overhead**: Network round-trips for every AI operation, even basic pattern matching
- **Offline Limitations**: No AI capabilities when external services are unavailable
- **Privacy Concerns**: All context processing requires data transmission to external services
- **Scalability Constraints**: API rate limits and costs limit the scale of AI-enhanced operations
- **Dependency on External Services**: Complete reliance on third-party APIs for all AI functionality

## Proposed Solution (Revised)

Integrate embedded LLM capabilities into Rhema's existing AI service architecture, creating a **hybrid AI processing system** that intelligently routes tasks between local embedded models and external APIs based on complexity, cost, and performance requirements.

### Key Changes from Original Proposal

1. **Leverage Existing Infrastructure**: Build on the sophisticated AI service already implemented
2. **Modern Embedded Models**: Focus on recent developments like Ollama, llama.cpp, and Candle
3. **Simplified Architecture**: Reduce complexity by leveraging existing systems
4. **Incremental Implementation**: Phase-based approach that builds on current capabilities

## Core Architecture (Revised)

### A. Enhanced AI Service Architecture

```rust
// Extend existing AIServiceConfig
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIServiceConfig {
    // ... existing fields ...
    
    // Embedded model configuration
    pub embedded: Option<EmbeddedModelConfig>,
    
    // Routing configuration
    pub routing: Option<TaskRoutingConfig>,
    
    // Fallback configuration
    pub fallback: Option<FallbackConfig>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbeddedModelConfig {
    pub enabled: bool,
    pub model_type: EmbeddedModelType,
    pub model_path: Option<PathBuf>,
    pub inference_engine: InferenceEngine,
    pub max_context_length: usize,
    pub memory_limit_mb: usize,
    pub enable_gpu: bool,
    pub gpu_memory_limit_mb: Option<usize>,
    pub quantization: QuantizationLevel,
    pub batch_size: usize,
    pub enable_caching: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum EmbeddedModelType {
    Ollama(String),      // e.g., "codellama:7b", "llama2:7b"
    LlamaCpp(PathBuf),   // Direct GGUF file path
    Candle(PathBuf),     // Candle model path
    Custom(String),      // Custom model identifier
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum InferenceEngine {
    Ollama,
    LlamaCpp,
    Candle,
    Custom(String),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum QuantizationLevel {
    Q2,
    Q4,
    Q8,
    F16,
    F32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskRoutingConfig {
    pub complexity_thresholds: ComplexityThresholds,
    pub cost_thresholds: CostThresholds,
    pub performance_thresholds: PerformanceThresholds,
    pub routing_strategy: RoutingStrategy,
    pub enable_hybrid_processing: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComplexityThresholds {
    pub simple_max_tokens: u32,
    pub medium_max_tokens: u32,
    pub simple_patterns: Vec<String>,
    pub complex_patterns: Vec<String>,
    pub context_query_threshold: u32,
    pub validation_threshold: u32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CostThresholds {
    pub max_cost_per_request: f64,
    pub max_daily_cost: f64,
    pub embedded_cost_multiplier: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceThresholds {
    pub max_response_time_ms: u64,
    pub min_accuracy_threshold: f64,
    pub embedded_timeout_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RoutingStrategy {
    EmbeddedFirst,
    ExternalFirst,
    Hybrid,
    CostOptimized,
    PerformanceOptimized,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FallbackConfig {
    pub enable_fallback: bool,
    pub fallback_threshold: f64,
    pub max_fallback_attempts: u32,
    pub fallback_strategy: FallbackStrategy,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum FallbackStrategy {
    ToExternal,
    ToEmbedded,
    ToHybrid,
    Fail,
}
```

### B. Task Classification System

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TaskComplexity {
    Simple,    // Use embedded model (context queries, validation, basic patterns)
    Medium,    // Hybrid approach (embedded preprocessing + external enhancement)
    Complex,   // External API only (code generation, complex reasoning, large contexts)
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskClassifier {
    pub complexity_rules: Vec<ComplexityRule>,
    pub pattern_matchers: Vec<PatternMatcher>,
    pub token_estimators: Vec<TokenEstimator>,
    pub context_analyzers: Vec<ContextAnalyzer>,
}

impl TaskClassifier {
    pub async fn classify_task(&self, request: &AIRequest) -> TaskComplexity {
        // Analyze prompt content, length, and patterns
        let token_count = self.estimate_tokens(&request.prompt).await;
        let pattern_complexity = self.analyze_patterns(&request.prompt).await;
        let context_complexity = self.analyze_context(&request).await;
        let task_type = self.analyze_task_type(&request).await;
        
        // Apply classification rules
        match task_type {
            TaskType::ContextQuery if token_count <= self.config.context_query_threshold => {
                TaskComplexity::Simple
            },
            TaskType::Validation if token_count <= self.config.validation_threshold => {
                TaskComplexity::Simple
            },
            TaskType::CodeGeneration if token_count <= self.config.simple_max_tokens => {
                TaskComplexity::Medium
            },
            _ if token_count <= self.config.simple_max_tokens && pattern_complexity.is_simple() => {
                TaskComplexity::Simple
            },
            _ if token_count <= self.config.medium_max_tokens && !pattern_complexity.is_complex() => {
                TaskComplexity::Medium
            },
            _ => TaskComplexity::Complex,
        }
    }
}
```

### C. Enhanced AI Service with Embedded Support

```rust
// Extend existing AIService
pub struct AIService {
    // ... existing fields ...
    
    // Embedded model components
    embedded_service: Option<Arc<EmbeddedLLMService>>,
    task_classifier: Option<Arc<TaskClassifier>>,
    model_manager: Option<Arc<ModelManager>>,
    performance_monitor: Option<Arc<PerformanceMonitor>>,
    cost_tracker: Option<Arc<CostTracker>>,
}

impl AIService {
    // ... existing methods ...
    
    pub async fn process_request(&self, request: AIRequest) -> RhemaResult<AIResponse> {
        // Check if embedded processing is available
        if let (Some(embedded_service), Some(task_classifier)) = 
            (&self.embedded_service, &self.task_classifier) {
            
            let complexity = task_classifier.classify_task(&request).await;
            
            match complexity {
                TaskComplexity::Simple => {
                    // Use embedded model for simple tasks
                    embedded_service.process_request(request).await
                },
                TaskComplexity::Medium => {
                    // Hybrid processing: embedded preprocessing + external enhancement
                    self.hybrid_process(request).await
                },
                TaskComplexity::Complex => {
                    // Use external API for complex tasks
                    self.call_external_api(request).await
                },
            }
        } else {
            // Fallback to external API only
            self.call_external_api(request).await
        }
    }
    
    async fn hybrid_process(&self, request: AIRequest) -> RhemaResult<AIResponse> {
        if let Some(embedded_service) = &self.embedded_service {
            // Step 1: Use embedded model for initial processing
            let embedded_response = embedded_service.process_request(request.clone()).await?;
            
            // Step 2: Enhance with external API if needed
            if self.needs_enhancement(&embedded_response) {
                let enhanced_request = self.create_enhancement_request(request, &embedded_response);
                let external_response = self.call_external_api(enhanced_request).await?;
                
                // Step 3: Combine results
                self.combine_responses(embedded_response, external_response).await
            } else {
                Ok(embedded_response)
            }
        } else {
            // Fallback to external API
            self.call_external_api(request).await
        }
    }
}
```

## Implementation Architecture (Revised)

### A. Embedded LLM Service

```rust
pub struct EmbeddedLLMService {
    config: EmbeddedModelConfig,
    model: Arc<dyn InferenceModel>,
    tokenizer: Arc<dyn Tokenizer>,
    cache: Arc<TimedCache<String, AIResponse>>,
    metrics: Arc<RwLock<EmbeddedMetrics>>,
    performance_monitor: Arc<PerformanceMonitor>,
}

#[async_trait]
pub trait InferenceModel: Send + Sync {
    async fn generate(&self, prompt: &str, max_tokens: u32, temperature: f32) -> RhemaResult<String>;
    async fn health_check(&self) -> RhemaResult<()>;
    async fn get_model_info(&self) -> ModelInfo;
}

pub struct OllamaModel {
    client: reqwest::Client,
    model_name: String,
    base_url: String,
}

pub struct LlamaCppModel {
    model: llama_cpp::Model,
    context: llama_cpp::Context,
}

pub struct CandleModel {
    model: candle_core::DType,
    tokenizer: candle_tokenizers::Tokenizer,
}

impl EmbeddedLLMService {
    pub async fn new(config: EmbeddedModelConfig) -> RhemaResult<Self> {
        let model = Self::load_model(&config).await?;
        let tokenizer = Self::load_tokenizer(&config).await?;
        
        Ok(Self {
            config,
            model: Arc::new(model),
            tokenizer: Arc::new(tokenizer),
            cache: Arc::new(TimedCache::with_lifespan(3600)),
            metrics: Arc::new(RwLock::new(EmbeddedMetrics::default())),
            performance_monitor: Arc::new(PerformanceMonitor::new()),
        })
    }
    
    pub async fn process_request(&self, request: AIRequest) -> RhemaResult<AIResponse> {
        let start_time = std::time::Instant::now();
        
        // Check cache first
        let cache_key = self.generate_cache_key(&request);
        if let Some(cached_response) = self.cache.get(&cache_key) {
            self.update_metrics(true, start_time.elapsed().as_millis() as u64, 0).await;
            return Ok(cached_response);
        }
        
        // Process with embedded model
        let content = self.model.generate(&request.prompt, request.max_tokens, request.temperature).await?;
        
        let response = AIResponse {
            id: Uuid::new_v4().to_string(),
            request_id: request.id.clone(),
            content,
            model_used: format!("embedded-{}", self.config.model_type),
            model_version: self.get_model_version().await?,
            tokens_used: self.estimate_tokens(&request.prompt),
            processing_time_ms: start_time.elapsed().as_millis() as u64,
            cached: false,
            created_at: Utc::now(),
            // ... existing fields ...
        };
        
        // Cache response
        self.cache.insert(cache_key, response.clone());
        
        // Update metrics
        self.update_metrics(false, response.processing_time_ms, response.tokens_used).await;
        
        Ok(response)
    }
}
```

### B. Model Management System

```rust
pub struct ModelManager {
    config: ModelManagerConfig,
    model_registry: Arc<RwLock<HashMap<String, ModelInfo>>>,
    download_manager: DownloadManager,
    model_loader: ModelLoader,
    ollama_client: Option<OllamaClient>,
}

impl ModelManager {
    pub async fn ensure_model_available(&self, model_type: &EmbeddedModelType) -> RhemaResult<()> {
        match model_type {
            EmbeddedModelType::Ollama(model_name) => {
                self.ensure_ollama_model(model_name).await
            },
            EmbeddedModelType::LlamaCpp(path) => {
                self.ensure_llama_cpp_model(path).await
            },
            EmbeddedModelType::Candle(path) => {
                self.ensure_candle_model(path).await
            },
            EmbeddedModelType::Custom(id) => {
                self.ensure_custom_model(id).await
            },
        }
    }
    
    async fn ensure_ollama_model(&self, model_name: &str) -> RhemaResult<()> {
        if let Some(client) = &self.ollama_client {
            // Check if model exists
            if !client.model_exists(model_name).await? {
                // Pull model
                client.pull_model(model_name).await?;
            }
        }
        Ok(())
    }
}
```

### C. Performance Monitoring and Optimization

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbeddedMetrics {
    pub total_requests: u64,
    pub successful_requests: u64,
    pub failed_requests: u64,
    pub cache_hits: u64,
    pub cache_misses: u64,
    pub average_response_time_ms: u64,
    pub total_tokens_processed: u64,
    pub memory_usage_mb: u64,
    pub gpu_utilization_percent: Option<f32>,
    pub model_load_time_ms: u64,
    pub last_updated: DateTime<Utc>,
    // Embedded-specific metrics
    pub embedded_requests: u64,
    pub hybrid_requests: u64,
    pub fallback_requests: u64,
    pub model_switches: u64,
}

pub struct PerformanceOptimizer {
    metrics: Arc<RwLock<EmbeddedMetrics>>,
    config: PerformanceConfig,
}

impl PerformanceOptimizer {
    pub async fn optimize_model_loading(&self) -> RhemaResult<()> {
        // Implement model preloading strategies
        // Optimize memory usage
        // Implement dynamic model unloading
        Ok(())
    }
    
    pub async fn monitor_performance(&self) -> RhemaResult<PerformanceReport> {
        let metrics = self.metrics.read().await;
        
        Ok(PerformanceReport {
            throughput: self.calculate_throughput(&metrics),
            latency: self.calculate_latency(&metrics),
            resource_usage: self.calculate_resource_usage(&metrics),
            recommendations: self.generate_recommendations(&metrics),
        })
    }
}
```

## Integration with Existing Systems (Updated)

### A. MCP Daemon Enhancement

```rust
// Extend existing McpDaemon
pub struct McpDaemon {
    // ... existing fields ...
    
    // Embedded AI components
    embedded_ai_service: Option<Arc<EmbeddedLLMService>>,
    hybrid_ai_router: Option<Arc<HybridAITaskRouter>>,
    context_processor: Option<Arc<EmbeddedContextProcessor>>,
}

impl McpDaemon {
    // ... existing methods ...
    
    pub async fn process_context_query(&self, query: ContextQuery) -> RhemaResult<ContextResponse> {
        // Use embedded model for real-time context processing
        if let Some(processor) = &self.context_processor {
            let processed_query = processor.enhance_query(query).await?;
            
            // Route to appropriate AI service based on complexity
            if let Some(router) = &self.hybrid_ai_router {
                let ai_response = router.route_task(processed_query).await?;
                self.context_provider.process_response(ai_response).await
            } else {
                // Fallback to existing context provider
                self.context_provider.process_query(processed_query).await
            }
        } else {
            // Fallback to existing context provider
            self.context_provider.process_query(query).await
        }
    }
    
    pub async fn evaluate_task_constraints(&self, task: &Task) -> RhemaResult<ConstraintEvaluation> {
        // Use embedded model for real-time constraint evaluation
        if let Some(processor) = &self.context_processor {
            processor.evaluate_constraints(task).await
        } else {
            // Fallback to existing constraint system
            self.constraint_system.evaluate_constraints(task).await
        }
    }
}
```

### B. Task Scoring Integration

```rust
// Extend existing task scoring system
pub struct EnhancedTaskScorer {
    embedded_evaluator: Option<Arc<EmbeddedTaskEvaluator>>,
    external_evaluator: ExternalTaskEvaluator,
    hybrid_router: Option<Arc<HybridAITaskRouter>>,
}

impl EnhancedTaskScorer {
    pub async fn score_task(&self, task: &Task) -> RhemaResult<TaskScore> {
        // Use embedded model for initial scoring
        if let Some(evaluator) = &self.embedded_evaluator {
            let embedded_score = evaluator.score_task(task).await?;
            
            // Enhance with external evaluation if needed
            if self.needs_external_evaluation(task, &embedded_score) {
                let external_score = self.external_evaluator.score_task(task).await?;
                self.combine_scores(embedded_score, external_score).await
            } else {
                Ok(embedded_score)
            }
        } else {
            // Fallback to external evaluation
            self.external_evaluator.score_task(task).await
        }
    }
    
    pub async fn evaluate_constraints(&self, task: &Task) -> RhemaResult<ConstraintEvaluation> {
        // Real-time constraint evaluation using embedded model
        if let Some(evaluator) = &self.embedded_evaluator {
            evaluator.evaluate_constraints(task).await
        } else {
            // Fallback to existing constraint system
            self.constraint_system.evaluate_constraints(task).await
        }
    }
}
```

### C. Context Bootstrapping Enhancement

```rust
pub struct EnhancedContextBootstrapper {
    embedded_processor: Option<Arc<EmbeddedContextProcessor>>,
    external_processor: ExternalContextProcessor,
    hybrid_router: Option<Arc<HybridAITaskRouter>>,
}

impl EnhancedContextBootstrapper {
    pub async fn bootstrap_context(&self, scope: &Scope) -> RhemaResult<BootstrappedContext> {
        // Use embedded model for local context processing
        if let Some(processor) = &self.embedded_processor {
            let local_context = processor.process_scope(scope).await?;
            
            // Enhance with external processing if needed
            if self.needs_external_enhancement(&local_context) {
                let enhanced_context = self.external_processor.enhance_context(local_context).await?;
                Ok(enhanced_context)
            } else {
                Ok(local_context)
            }
        } else {
            // Fallback to external processing
            self.external_processor.process_scope(scope).await
        }
    }
    
    pub async fn generate_primer(&self, context: &BootstrappedContext) -> RhemaResult<String> {
        // Use embedded model for primer generation
        if let Some(processor) = &self.embedded_processor {
            processor.generate_primer(context).await
        } else {
            // Fallback to external processor
            self.external_processor.generate_primer(context).await
        }
    }
}
```

## Implementation Roadmap (Revised)

### Phase 1: Foundation (2-3 weeks)

**Week 1: Core Infrastructure**
- Extend existing `AIServiceConfig` with embedded model configuration
- Implement basic embedded LLM service using Ollama integration
- Create model management system for Ollama models
- Add basic task classification system

**Week 2-3: Integration Framework**
- Implement intelligent task router
- Create hybrid processing capabilities
- Add performance monitoring and metrics collection
- Implement caching for embedded model responses

### Phase 2: Enhanced Integration (2-3 weeks)

**Week 4-5: MCP Daemon Enhancement**
- Extend MCP daemon with embedded LLM capabilities
- Integrate with existing context provider
- Add real-time context processing capabilities
- Implement embedded constraint evaluation

**Week 6: Task Scoring Integration**
- Enhance task scoring system with embedded evaluation
- Add real-time constraint checking using embedded models
- Implement hybrid task scoring strategies
- Add performance optimization for task evaluation

### Phase 3: Advanced Features (2-3 weeks)

**Week 7-8: Context Bootstrapping Enhancement**
- Extend context bootstrapping with embedded processing
- Add local context summarization and pattern extraction
- Implement hybrid context enhancement strategies
- Add offline context processing capabilities

**Week 9: Performance Optimization**
- Implement advanced caching strategies
- Add dynamic model loading/unloading
- Optimize memory usage and GPU utilization
- Add comprehensive performance monitoring

### Phase 4: Production Readiness (1-2 weeks)

**Week 10: Testing and Validation**
- Comprehensive testing of embedded model capabilities
- Performance benchmarking and optimization
- Quality assurance and fallback testing
- Security and privacy validation

**Week 11-12: Documentation and Deployment**
- Complete documentation and user guides
- Production deployment preparation
- Monitoring and alerting setup
- Training and support materials

## CLI Commands (Updated)

```bash
# Embedded model management
rhema ai models list --embedded              # List available embedded models
rhema ai models install --model codellama:7b # Install embedded model via Ollama
rhema ai models update --model codellama:7b  # Update embedded model
rhema ai models remove --model codellama:7b  # Remove embedded model
rhema ai models validate --model codellama:7b # Validate model integrity
rhema ai models status                       # Show model status and health

# Hybrid AI service management
rhema ai service status                      # Show hybrid service status
rhema ai service config --show               # Show current configuration
rhema ai service config --update config.yaml # Update configuration
rhema ai service restart                     # Restart hybrid service
rhema ai service health                      # Check service health

# Task routing and classification
rhema ai tasks classify --prompt "..."       # Classify task complexity
rhema ai tasks route --task task.yaml        # Route task to appropriate service
rhema ai tasks benchmark --service embedded  # Benchmark embedded service
rhema ai tasks benchmark --service external  # Benchmark external service
rhema ai tasks benchmark --service hybrid    # Benchmark hybrid service

# Performance monitoring
rhema ai performance metrics --embedded      # Show embedded model metrics
rhema ai performance metrics --external      # Show external API metrics
rhema ai performance compare                 # Compare embedded vs external
rhema ai performance optimize                # Optimize performance settings
rhema ai performance health                  # Check performance health

# Cost analysis
rhema ai costs analyze --period week         # Analyze costs for period
rhema ai costs optimize --strategy hybrid    # Optimize cost strategy
rhema ai costs project --usage heavy         # Project costs for usage pattern
rhema ai costs savings --show                # Show cost savings from hybrid approach

# Ollama integration
rhema ai ollama list                         # List available Ollama models
rhema ai ollama pull --model codellama:7b    # Pull Ollama model
rhema ai ollama remove --model codellama:7b  # Remove Ollama model
rhema ai ollama status                       # Check Ollama service status
```

## Configuration Examples (Updated)

### Basic Hybrid Configuration

```yaml
# .rhema/ai-config.yaml
ai_service:
  # Existing external configuration
  api_key: "${OPENAI_API_KEY}"
  base_url: "https://api.openai.com"
  timeout_seconds: 30
  max_concurrent_requests: 100
  rate_limit_per_minute: 300
  cache_ttl_seconds: 3600
  model_version: "gpt-4"
  enable_caching: true
  enable_rate_limiting: true
  enable_monitoring: true
  
  # New embedded configuration
  embedded:
    enabled: true
    model_type: "Ollama"
    model_name: "codellama:7b"
    inference_engine: "Ollama"
    max_context_length: 4096
    memory_limit_mb: 8192
    enable_gpu: true
    gpu_memory_limit_mb: 4096
    quantization: "Q4"
    batch_size: 1
    enable_caching: true
  
  # New routing configuration
  routing:
    complexity_thresholds:
      simple_max_tokens: 512
      medium_max_tokens: 2048
      context_query_threshold: 256
      validation_threshold: 128
      simple_patterns:
        - "context query"
        - "validation"
        - "basic pattern"
        - "check"
        - "verify"
      complex_patterns:
        - "code generation"
        - "complex reasoning"
        - "large context"
        - "analysis"
        - "optimization"
    cost_thresholds:
      max_cost_per_request: 0.10
      max_daily_cost: 10.0
      embedded_cost_multiplier: 0.1
    performance_thresholds:
      max_response_time_ms: 5000
      min_accuracy_threshold: 0.8
      embedded_timeout_ms: 3000
    routing_strategy: "Hybrid"
    enable_hybrid_processing: true
  
  # New fallback configuration
  fallback:
    enable_fallback: true
    fallback_threshold: 0.7
    max_fallback_attempts: 3
    fallback_strategy: "ToExternal"
```

### Advanced Configuration

```yaml
# .rhema/ai-config-advanced.yaml
ai_service:
  # Multiple external providers
  external:
    providers:
      openai:
        api_key: "${OPENAI_API_KEY}"
        base_url: "https://api.openai.com"
        models: ["gpt-4", "gpt-3.5-turbo"]
        priority: 1
      anthropic:
        api_key: "${ANTHROPIC_API_KEY}"
        base_url: "https://api.anthropic.com"
        models: ["claude-3-opus", "claude-3-sonnet"]
        priority: 2
  
  # Multiple embedded models
  embedded:
    enabled: true
    models:
      codellama-7b:
        model_type: "Ollama"
        model_name: "codellama:7b"
        inference_engine: "Ollama"
        max_context_length: 4096
        use_case: "code_generation"
        priority: 1
      phi-2:
        model_type: "Ollama"
        model_name: "phi:2"
        inference_engine: "Ollama"
        max_context_length: 2048
        use_case: "general_reasoning"
        priority: 2
      llama2-7b:
        model_type: "Ollama"
        model_name: "llama2:7b"
        inference_engine: "Ollama"
        max_context_length: 4096
        use_case: "conversation"
        priority: 3
  
  # Advanced routing rules
  routing:
    rules:
      - name: "code_generation"
        condition: "prompt contains 'generate code' or 'write function'"
        action: "use embedded codellama-7b"
        fallback: "external gpt-4"
      
      - name: "context_query"
        condition: "prompt contains 'context' and tokens < 512"
        action: "use embedded phi-2"
        fallback: "external gpt-3.5-turbo"
      
      - name: "complex_reasoning"
        condition: "tokens > 2048 or complexity > 0.8"
        action: "use external gpt-4"
        fallback: "hybrid processing"
    
    # Performance-based routing
    performance_routing:
      enable: true
      metrics:
        - "response_time"
        - "accuracy"
        - "cost"
      thresholds:
        response_time_ms: 3000
        accuracy_threshold: 0.85
        cost_threshold: 0.05
```

## Success Metrics (Updated)

### Technical Metrics

- **Response Time**: 70% reduction in average response time for simple tasks
- **Cost Efficiency**: 60% reduction in AI processing costs
- **Availability**: 99.9% uptime with offline capabilities
- **Accuracy**: Maintain >95% accuracy compared to external APIs for simple tasks
- **Model Loading**: <5 second model load time for embedded models
- **Memory Usage**: <4GB memory usage for embedded models

### User Experience Metrics

- **Latency**: <500ms response time for embedded tasks
- **Reliability**: <1% failure rate for embedded processing
- **Transparency**: Clear indication of which service processed each request
- **Flexibility**: Seamless switching between embedded and external processing
- **Offline Capability**: Full functionality without internet connection

### Business Metrics

- **Cost Savings**: 50% reduction in AI processing costs
- **Performance**: 3x improvement in task throughput
- **Scalability**: Support for 10x more concurrent AI operations
- **Privacy**: 100% local processing for sensitive operations
- **ROI**: Positive ROI within 3 months of deployment

## Recent Developments and Considerations

### 2024-2025 Embedded LLM Landscape

1. **Ollama**: Dominant platform for local model management
2. **llama.cpp**: Mature inference engine with broad model support
3. **Candle**: Rust-native inference engine gaining popularity
4. **Model Quantization**: Q2, Q4, Q8 quantization widely available
5. **Hardware Acceleration**: Better GPU support across platforms

### Technology Choices

1. **Primary Engine**: Ollama for ease of use and model management
2. **Fallback Engine**: llama.cpp for direct GGUF file support
3. **Future Engine**: Candle for Rust-native performance
4. **Model Format**: GGUF for broad compatibility
5. **Quantization**: Q4 for optimal performance/size balance

### Integration Considerations

1. **Existing Infrastructure**: Leverage sophisticated AI service already built
2. **Incremental Deployment**: Phase-based approach to minimize risk
3. **Fallback Strategies**: Robust fallback to external APIs
4. **Performance Monitoring**: Comprehensive metrics and alerting
5. **Security**: Local processing for sensitive operations

## Future Enhancements (Updated)

### A. Advanced Model Management

- **Dynamic model loading**: Load models on-demand based on usage patterns
- **Model versioning**: Automatic model updates and rollback capabilities
- **Multi-model ensemble**: Combine multiple embedded models for better results
- **Custom model training**: Fine-tune models on project-specific data
- **Model compression**: Advanced compression techniques for smaller models

### B. Intelligent Optimization

- **Adaptive routing**: Learn optimal routing strategies from usage patterns
- **Predictive caching**: Pre-cache likely requests based on context
- **Resource optimization**: Dynamic memory and GPU allocation
- **Quality monitoring**: Continuous model performance evaluation
- **Auto-scaling**: Automatic model scaling based on demand

### C. Ecosystem Integration

- **IDE integration**: Real-time embedded AI assistance in development environments
- **CI/CD integration**: Automated code review and testing with embedded models
- **Monitoring integration**: Comprehensive observability and alerting
- **Analytics integration**: Detailed usage analytics and optimization insights
- **Plugin ecosystem**: Third-party model and engine support

## Conclusion

This revised proposal builds upon Rhema's existing sophisticated AI service architecture to add embedded LLM capabilities. The implementation is now more focused, leveraging existing infrastructure while adding the missing embedded model components. The timeline has been reduced from 16 weeks to 8-12 weeks due to the existing foundation.

The hybrid approach will provide significant benefits:
- **Cost reduction** through intelligent task routing
- **Performance improvement** with local processing for simple tasks
- **Offline capabilities** for critical operations
- **Privacy enhancement** for sensitive data processing
- **Scalability improvements** through reduced external API dependency

This embedded LLM integration will transform Rhema into a **comprehensive intelligent development platform** that provides optimal AI capabilities for every type of task, from simple context queries to complex code generation, while maintaining cost efficiency and performance excellence. 